{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f310348-43f4-44f0-bc9f-753bdbd899da",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31cf72-0d0c-4642-b039-3d65c0579568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"src/\")\n",
    "from evaluate import EvaluationResult, ModelResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d641cf-70cc-415d-9875-37c54e6f2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"Fine-tuning LLMs blog post - Raw Data - Results.csv\"\n",
    "import pandas as pd\n",
    "\n",
    "data_file = pd.read_csv(DATA_FILE, header=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13244f83-25dd-4386-94f6-641f44a8bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ROOT_DIR = \"../data/\"\n",
    "\n",
    "results = data_file[\"Result Path\"].dropna().replace(np.nan, None).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe07a30-3a5c-474a-8ee5-3ab056d23992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(label : str, df : pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[ df.iloc[:, 0] == label ]\n",
    "\n",
    "def parse_classification_report(df : pd.DataFrame) -> dict:\n",
    "\n",
    "    report = {}\n",
    "    labels = df.iloc[:, 0].to_list()\n",
    "\n",
    "    for label in labels:\n",
    "        values = get_values(label, df)\n",
    "        if label == 'accuracy':\n",
    "            accuracy = values['precision'].to_list()[0]\n",
    "            report[label] = accuracy\n",
    "        else:\n",
    "            precision, recall, f1, support = values[['precision', 'recall', 'f1-score', 'support']].iloc[0].to_list()\n",
    "            report[label] = {\n",
    "                'precision' : precision,\n",
    "                'recall' : recall,\n",
    "                'f1' : f1,\n",
    "                'support' : support\n",
    "            }\n",
    "    \n",
    "    return report\n",
    "\n",
    "def collect_average_metrics(results : dict) -> dict:\n",
    "    avgs = ['macro avg', 'weighted avg']\n",
    "    metrics = ['precision', 'recall', 'f1']\n",
    "    \n",
    "    avg_metrics = {}\n",
    "    avg_metrics['accuracy'] = float(np.mean([i['accuracy'] for i in results.values()]))\n",
    "    \n",
    "    for avg in avgs:\n",
    "        for metric in metrics:\n",
    "\n",
    "            values = [i[avg] for i in results.values()]\n",
    "            values = [i[metric] for i in values]\n",
    "            avg_metrics[metric + \" - \" + avg] = float(np.mean(values))\n",
    "\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0383dff-6726-4ec7-89c0-1e3206b7af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "evaluations = {}\n",
    "\n",
    "def parse_raw_output(raw_output_file : str) -> EvaluationResult:\n",
    "    # The file format of EvaluationResults changed since I started\n",
    "    # collecting data, so this function updates the outdated\n",
    "    # EvaluationResult dict objects to match the new standard\n",
    "    # before they are parsed to prevent an Exception.\n",
    "    \n",
    "    with open(raw_output_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "    data[\"total_tokens_per_response\"] = None\n",
    "    data[\"total_tokens\"] = None\n",
    "    if data[\"config\"].get(\"name\"):\n",
    "        data[\"config\"][\"technique_name\"] = data[\"config\"][\"name\"]\n",
    "        del data[\"config\"][\"name\"]\n",
    "\n",
    "    fixed_responses = []\n",
    "    for response in data[\"llm_responses\"]:\n",
    "        if type(response) is str:\n",
    "            fixed_responses.append(ModelResponse(text=response,prompt_tokens=0,completion_tokens=0,total_tokens=0,latency=0).to_dict())\n",
    "        else: fixed_responses.append(response)\n",
    "    data[\"llm_responses\"] = fixed_responses\n",
    "\n",
    "    eval_result = EvaluationResult.from_dict(data)\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05daaac-cf96-4472-b1cd-828865185eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    if result is None: continue\n",
    "    \n",
    "    path = os.path.join(ROOT_DIR, result)\n",
    "    \n",
    "    classification_reports = glob.glob( os.path.join(path, \"evaluation_*.csv\") )\n",
    "    \n",
    "    if not classification_reports:\n",
    "        print(f\"Could not find results at {path}.\")\n",
    "        continue\n",
    "    \n",
    "    raw_output_file = glob.glob( os.path.join(path, \"raw_output.json\") )[0]\n",
    "\n",
    "    eval_result = parse_raw_output(raw_output_file)\n",
    "\n",
    "    num_samples = int(len(eval_result.texts))\n",
    "    total_latency = eval_result.total_time_elapsed\n",
    "    \n",
    "    latency = total_latency / num_samples\n",
    "        \n",
    "    evaluation = {}\n",
    "    for report in classification_reports:\n",
    "        label_name = re.findall(r\"evaluation_(.*).csv\", report)[0]\n",
    "\n",
    "        data = pd.read_csv(report)\n",
    "        data = parse_classification_report(data)\n",
    "        evaluation[label_name] = data\n",
    "\n",
    "    avg_metrics = collect_average_metrics(evaluation)\n",
    "\n",
    "    # Insert latency at position 1 of the dict\n",
    "    avg_metrics = list(avg_metrics.items())\n",
    "    avg_metrics.insert(1, (\"latency\", latency))\n",
    "    avg_metrics.insert(0, (\"samples\", num_samples))\n",
    "    avg_metrics.insert(0, (\"max tokens\", int(eval_result.config.max_tokens)))\n",
    "    avg_metrics.insert(0, (\"prompt\", eval_result.config.prompt))\n",
    "\n",
    "    avg_metrics.insert(0, (\"total_tokens\", sum([i.total_tokens for i in eval_result.llm_responses])))\n",
    "    avg_metrics.insert(0, (\"completion_tokens\", sum([i.completion_tokens for i in eval_result.llm_responses])))\n",
    "    avg_metrics.insert(0, (\"prompt_tokens\", sum([i.prompt_tokens for i in eval_result.llm_responses])))\n",
    "    avg_metrics = dict(avg_metrics)\n",
    "\n",
    "    evaluations[result] = avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b91c4c-4b7b-4710-8e77-98e224c5cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9affb3-5849-4f9b-be44-815e70f45b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(evaluations).transpose()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e2d57-cce4-4917-a6e0-b44d82c40214",
   "metadata": {},
   "source": [
    "# Few-shot prompt generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8065f6c-0d02-40c6-a119-38d7b380bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "sys.path.append(\"../src/\")\n",
    "import evaluate as ev\n",
    "import preprocess as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c87e8-509f-4cd6-8964-4a0458ab2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(results):\n",
    "    print(f\"{i} - {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325debe4-8b68-49bd-b1af-2c6826b77c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = results[10]\n",
    "import json\n",
    "with open( os.path.join(ROOT_DIR, os.path.join(file, \"raw_output.json\")) ) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Update data structure from previous version\n",
    "if not data['config'].get(\"technique_name\"):\n",
    "    data['config']['technique_name'] = data['config']['name']\n",
    "    del data['config']['name']\n",
    "for i in ['label_names', 'labels_pred', 'labels_true']:\n",
    "    if type(data[i]) is list:\n",
    "        data[i] = {\"NatureTitle\":data[i]}\n",
    "\n",
    "data = ev.EvaluationResult.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd6037-aeab-44fc-b458-8f0f4798df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = data.get_few_shot_examples(3,1,1)\n",
    "\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131c530-b51a-4b49-8f75-47c98d2587c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = \"Final Narrative\"\n",
    "output_labels = [\"NatureTitle\", \"Part of Body Title\"]\n",
    "\n",
    "# Preprocess the dataset into a form usable for supervised finetuning\n",
    "\n",
    "dataset, label_names = pre.load_dataset(\n",
    "    os.path.join(ROOT_DIR, \"osha/datasets/imbalanced_multiclass_train.csv\"),\n",
    "    input_features,\n",
    "    output_labels,\n",
    "    test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cd06f-3768-46d1-a5f1-88761924280c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = ev.create_prompt(\"OSHA injury report\", label_names, examples=examples)\n",
    "\n",
    "print(prompt.replace(\"\\n\", \"\\\\n\").replace('\"', '\\\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abdce4-acb1-4865-9d87-6d3376969493",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531340ed-3642-4dc9-9b4f-804b9e3dec0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
