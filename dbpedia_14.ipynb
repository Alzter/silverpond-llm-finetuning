{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–¶ï¸ Configure GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"#\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKRSEolKGDiw"
   },
   "source": [
    "# â–¶ï¸ Load and Preprocess DBPedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCnfFuWPWr9-"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fancyzhx/dbpedia_14\")\n",
    "CLASS_LABELS = dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY442nBwWr-A"
   },
   "source": [
    "## Reduce dataset size via sampling\n",
    "Let's obtain the first **n** samples from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4j8rSsCWr-D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_n_samples_per_class(dataset, n, shuffle = False, seed=0):\n",
    "    \"\"\"\n",
    "        Given a dataset, obtain the first n samples from each class\n",
    "        and return a smaller dataset containing all the samples.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to sample.\n",
    "            n (int): How many samples from each class to extract.\n",
    "            shuffle (bool): Whether to sort the final result by class or randomly. NOTE: Dataset.shuffle() hangs indefinitely on Nix.\n",
    "\n",
    "        Returns:\n",
    "            sample (Dataset): The sampled dataset.\n",
    "    \"\"\"\n",
    "    ds_sorted = dataset.sort('label')\n",
    "    _, class_indices = np.unique(ds_sorted['label'], return_index=True)\n",
    "\n",
    "    class_indices = np.array([list(range(index, index + n)) for index in class_indices])\n",
    "    class_indices = class_indices.flatten()\n",
    "\n",
    "    if shuffle:\n",
    "        sample = dataset.shuffle(seed=seed).sort('label').select(class_indices) # Dataset.shuffle() hangs indefinitely on Nix - No idea why.\n",
    "    else:\n",
    "        sample = dataset.sort('label').select(class_indices)\n",
    "\n",
    "    if shuffle: sample = sample.shuffle(seed=seed) # Dataset.shuffle() hangs indefinitely on Nix - No idea why.\n",
    "    return sample\n",
    "\n",
    "def sample_dataset(dataset, ratio = None, size = None, samples_per_class = None, seed=0):\n",
    "    \"\"\"\n",
    "        Given a dataset, return a smaller dataset with an\n",
    "        equal number of samples per class. You can specify\n",
    "        the size of the new dataset directly (size), or\n",
    "        using a number of samples per class (samples_per_class),\n",
    "        or as a percentage of the original dataset (ratio).\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to sample.\n",
    "            ratio (float, optional): What percentage of the dataset to sample.\n",
    "            size (int, optional): Number of samples the new dataset should have.\n",
    "            samples_per_class (int, optional): Number of samples per class the new dataset should have.\n",
    "            seed (int, optional): Random seed.\n",
    "\n",
    "        Returns:\n",
    "            sampled_dataset (Dataset): The smaller dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if ratio is None and size is None and samples_per_class is None:\n",
    "        raise ValueError(\"Either ratio, size, or samples_per_class must be given.\")\n",
    "\n",
    "    if samples_per_class is None:\n",
    "        if size is not None:\n",
    "            ratio = size / dataset.num_rows\n",
    "        ratio = max(ratio, 0)\n",
    "        ratio = min(ratio, 1)\n",
    "    \n",
    "        samples_per_class = dataset.num_rows // len(dataset.features['label'].names)\n",
    "        samples_per_class = int(samples_per_class * ratio)\n",
    "\n",
    "    return get_n_samples_per_class(dataset, samples_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configure Dataset Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}\n",
    "ds['train'] = sample_dataset(dataset['train'], ratio=0.1)\n",
    "ds['test'] = sample_dataset(dataset['test'], ratio=0.25)\n",
    "\n",
    "print(f\"Train: {len(ds['train'])} samples.\")\n",
    "print(f\"Test: {len(ds['test'])} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format as a supervised fine-tuning dataset\n",
    "\n",
    "To fine-tune our LLM, we will use the ``trl`` library with\n",
    "the ``SFTTrainer`` class. To do this, we must format our data\n",
    "in [conversational](https://huggingface.co/docs/trl/main/en/dataset_formats#conversational) format using chat templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Value\n",
    "\n",
    "def preprocess_dbpedia_sample(sample, class_labels):\n",
    "    \"\"\"\n",
    "        Given a sample in the dbpedia_14 dataset, convert it\n",
    "        to a format usable by SFTTrainer by substituting the\n",
    "        label ID with the label name. (\"0\" -> \"Company\")\n",
    "    \"\"\"\n",
    "    sample['label'] = class_labels[ int(sample['label'] )]\n",
    "    sample['content'] = sample['content'].strip()\n",
    "    return sample\n",
    "    \n",
    "def apply_chat_template(sample):\n",
    "    \"\"\"\n",
    "        Convert a standard Dataset to conversational.\n",
    "    \"\"\"\n",
    "    sample['messages'] = [\n",
    "        {\"role\":\"user\", \"content\":sample['prompt']},\n",
    "        {\"role\":\"assistant\", \"content\":sample['completion']}]\n",
    "    del sample['prompt']\n",
    "    del sample['completion']\n",
    "    return sample\n",
    "    \n",
    "def process_dbpedia_dataset(dataset, seed=0):\n",
    "    \"\"\"\n",
    "    Convert the dbpedia_14 dataset into the Hugging Face conversational dataset format.\n",
    "    This format is outlined here: https://huggingface.co/docs/trl/main/en/dataset_formats#conversational\n",
    "\n",
    "    This is needed to finetune our LLM, because the ``SFTTrainer``\n",
    "    class from ``trl`` requires a dataset in the conversational format.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to format.\n",
    "        seed (int): Random number seed.\n",
    "\n",
    "    Returns:\n",
    "        dataset (Dataset): The dataset in conversational format.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_names = dataset.features['label'].names\n",
    "    \n",
    "    # Change the label data type to string. (0 -> \"0\")\n",
    "    dataset = dataset.cast_column(\"label\", Value(dtype='string'))\n",
    "    # Substitute the label ID with the label name. (\"0\" -> \"Company\")\n",
    "    dataset = dataset.map( lambda x : preprocess_dbpedia_sample(x, label_names) )\n",
    "\n",
    "    dataset = dataset.rename_column(\"content\", \"prompt\")\n",
    "    dataset = dataset.rename_column(\"label\", \"completion\")\n",
    "    dataset = dataset.remove_columns([\"title\"])\n",
    "    dataset = dataset.map(apply_chat_template)\n",
    "\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'] = process_dbpedia_dataset(ds['train'])\n",
    "ds['test'] = process_dbpedia_dataset(ds['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coDi5e9WWr-K"
   },
   "source": [
    "# â–¶ï¸ Load Baseline LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_MAP = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpf3SAVJWr-L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Same quantization configuration as QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\", # QLoRA uses 4-bit NormalFloat precision,\n",
    "    bnb_4bit_use_double_quant = True, # QLoRA uses double quantising,\n",
    "    bnb_4bit_compute_dtype = torch.float32\n",
    ")\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=DEVICE_MAP)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Mb of RAM is the model using?\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–¶ï¸ Finetune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEFT_MODEL_NAME = \"Qwen2.5-FT-DBPedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_dimension = 6 # the rank of the adapter, the lower the fewer parameters you'll need to train. (smaller = more compression)\n",
    "lora_alpha = 8 # this is the scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_dropout = 0.05 # dropout probability for LoRA layers (helps prevent overfitting)\n",
    "max_seq_length = 10\n",
    "epochs=1\n",
    "learning_rate=2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LoRA adapters to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=rank_dimension,\n",
    "    lora_alpha=lora_alpha,\n",
    "    bias=\"none\",           # BEWARE: training biases *modifies* base model's behavior\n",
    "    lora_dropout=lora_dropout,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Mb of RAM is the model using?\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,\n",
    "    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,\n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=16,\n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "    \n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=64,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "    \n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    optim='adamw_torch_fused',\n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup    \n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir=PEFT_MODEL_NAME,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(PEFT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–¶ï¸ Load Finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_MAP = \"auto\"#\"cuda:0\"\n",
    "PEFT_MODEL_NAME = \"Qwen2.5-FT-DBPedia\"\n",
    "\n",
    "from peft import PeftConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL_NAME)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(PEFT_MODEL_NAME, device_map=DEVICE_MAP, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â–¶ï¸ LLM helper methods\n",
    "Conventional methods to generate text with ðŸ¤— Transformers models such as [TextGenerationPipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextGenerationPipeline) don't work with LoRA models, so we will use [Daniel Godoy's method](https://github.com/dvgodoy/FineTuningLLMs/blob/main/Chapter6.ipynb) for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    \"\"\"\n",
    "        Convert the user's query into conversational format.\n",
    "        Source: https://github.com/dvgodoy/FineTuningLLMs/blob/main/Chapter6.ipynb\n",
    "    \"\"\"\n",
    "    if type(sentence) is str:\n",
    "        sentence = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        sentence, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate(query, model, tokenizer, \n",
    "             max_new_tokens=64, \n",
    "             skip_special_tokens=True, \n",
    "             response_only=True,\n",
    "             do_sample=True,\n",
    "             temperature=0.1):\n",
    "    \"\"\"\n",
    "        Generate an LLM response to a user query.\n",
    "        Source: https://github.com/dvgodoy/FineTuningLLMs/blob/main/Chapter6.ipynb\n",
    "    \"\"\"\n",
    "    # Converts user query into a formatted prompt.\n",
    "    prompt=gen_prompt(tokenizer,query)\n",
    "    \n",
    "    # Tokenizes the formatted prompt\n",
    "    tokenized_input = tokenizer(prompt,\n",
    "                                add_special_tokens=False,\n",
    "                                return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # Generates the response/completion\n",
    "    generation_output = model.generate(**tokenized_input,\n",
    "                                       max_new_tokens=max_new_tokens,\n",
    "                                       do_sample=do_sample,\n",
    "                                       temperature=temperature)\n",
    "    \n",
    "    # If required, removes the tokens belonging to the prompt\n",
    "    if response_only:\n",
    "        input_length = tokenized_input['input_ids'].shape[1]\n",
    "        generation_output = generation_output[:, input_length:]\n",
    "    \n",
    "    # Decodes the tokens back into text\n",
    "    output = tokenizer.batch_decode(generation_output, \n",
    "                                    skip_special_tokens=skip_special_tokens)[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8Un3PF5fr-1"
   },
   "source": [
    "# â–¶ï¸ Evaluate LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qQAMgasWr-H"
   },
   "outputs": [],
   "source": [
    "from model_prompts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gq0uPzxWr-I"
   },
   "outputs": [],
   "source": [
    "def get_classification_prompt(article, prompt):\n",
    "    \"\"\"\n",
    "      For a given article in the Dataset,\n",
    "      return a LLM prompt in chat template form\n",
    "      to get its category.\n",
    "\n",
    "      Args:\n",
    "          article (Dictionary): Any item in the dataset.\n",
    "          prompt (str): A model prompt with article classification instructions.\n",
    "\n",
    "      Returns:\n",
    "          prompt (Dictionary): The prompt as a [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating).\n",
    "    \"\"\"\n",
    "\n",
    "    if prompt is not None and prompt != \"\":\n",
    "        prompt = {\"role\": \"system\", \"content\": prompt}\n",
    "    \n",
    "    return [\n",
    "      prompt,\n",
    "      {\"role\": \"user\", \"content\": article['messages'][0]['content'].strip()},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNLzAeHbR3X9"
   },
   "outputs": [],
   "source": [
    "CLASS_LABELS = ['Company',\n",
    " 'EducationalInstitution',\n",
    " 'Artist',\n",
    " 'Athlete',\n",
    " 'OfficeHolder',\n",
    " 'MeanOfTransportation',\n",
    " 'Building',\n",
    " 'NaturalPlace',\n",
    " 'Village',\n",
    " 'Animal',\n",
    " 'Plant',\n",
    " 'Album',\n",
    " 'Film',\n",
    " 'WrittenWork',\n",
    " 'Unknown'] # Final label is only used if LLM *cannot* predict the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_8az0xNY88d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def get_class_label_from_string(string, class_labels = CLASS_LABELS, unknown_label_strategy = \"last\"):\n",
    "    \"\"\"\n",
    "    Extract a class label by name from a string and return its ID.\n",
    "    If no match is found, choose a random label ID.\n",
    "\n",
    "    Args:\n",
    "    string (str): A string containing the name of one class label.\n",
    "    unknown_label_strategy (str): What to do if the string does not contain any class labels.\n",
    "                                    \"random\": Return a random class label.\n",
    "                                    \"last\": Return the last class label in the list.\n",
    "\n",
    "    Returns:\n",
    "    class_id (int): The ID of the matching class label.\n",
    "    \"\"\"\n",
    "\n",
    "    # UNKNOWN_LABEL_STRATEGIES = [\"random\", \"last\"]\n",
    "    # if not unknown_label_strategy in UNKNOWN_LABEL_STRATEGIES:\n",
    "    #     raise Exception(f\"unknown_label_strategy must be one of the following: {UNKNOWN_LABEL_STRATEGIES}\")\n",
    "    \n",
    "    # Return a direct match if possible\n",
    "    try:\n",
    "        class_id = class_labels.index(string)\n",
    "        return class_id\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    string = string.lower().strip()\n",
    "    \n",
    "    # Match class labels if string is truncated.\n",
    "    # E.g., \"mean\" -> \"meanoftransportation\" -> 5\n",
    "    # This allows us to match labels even we don't\n",
    "    # have enough tokens to write the entire name.\n",
    "    # This allows us to optimise the evaluation\n",
    "    # procedure by using lower max tokens.\n",
    "    for i, label in enumerate(class_labels):\n",
    "        \n",
    "        label_truncated = label.lower().replace(\" \", \"\")[0:len(string)]\n",
    "        if string == label_truncated:\n",
    "            # print(f\"truncated match: {string} = {label}\")\n",
    "            return i\n",
    "    \n",
    "    # Concatenate all label names using boolean OR.\n",
    "    match = \"|\".join(class_labels).lower().replace(\" \", r\"\\s*\")\n",
    "\n",
    "    # Find all instances of label name strings within the base string.\n",
    "    matches = re.findall(match, string)\n",
    "\n",
    "    # If no class label is found in the LLM text, pick a random label.\n",
    "    if matches == []:\n",
    "        # print(f\"not found: {string}\")\n",
    "        match unknown_label_strategy:\n",
    "            case \"random\":\n",
    "                return random.randint(0, len(class_labels) - 1)\n",
    "            case \"last\":\n",
    "                return len(class_labels) - 1\n",
    "\n",
    "    # Get the last matching label from the string.\n",
    "    final_match = matches[-1]\n",
    "\n",
    "    # Remove all capitalisation, non-alphabetic characters, and whitespace\n",
    "    labels_sanitised = [re.sub(\"[^a-z]\", \"\", label.lower()) for label in class_labels]\n",
    "    match_sanitised = re.sub(\"[^a-z]\", \"\", final_match.lower())\n",
    "\n",
    "    # Find the matching class ID for the label.\n",
    "    class_id = labels_sanitised.index(match_sanitised)\n",
    "    # print(f\"regex match: {string}\")\n",
    "    return class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3Dt-uW3dmLj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_first_number_from_string(string):\n",
    "  \"\"\"\n",
    "  Returns the first whole number from a string as an integer.\n",
    "  \"\"\"\n",
    "  first_number=re.findall(r\"\\d+\",string)\n",
    "  if first_number is not None:\n",
    "    first_number = int(first_number[0])\n",
    "    return first_number\n",
    "  else:\n",
    "    raise Exception(f\"No number found in string: {string}\")\n",
    "\n",
    "def get_category_label(article, model, tokenizer, classification_prompt, extractor_func, max_tokens = 10):\n",
    "  \"\"\"\n",
    "  For a given article in the DBPedia dataset, predict its category label.\n",
    "\n",
    "  Args:\n",
    "    article (str): Article contents as raw text.\n",
    "    classifiction_prompt (str): Model instructions on how to classify articles.\n",
    "    extractor_func (func): A method which takes the model's response and returns a classification label as an integer.\n",
    "    max_tokens (int): Model response word limit.\n",
    "\n",
    "  Returns:\n",
    "    output (tuple<int, str>): The category of the article and the raw LLM output.\n",
    "  \"\"\"\n",
    "  input_prompt = get_classification_prompt(article, classification_prompt)\n",
    "\n",
    "  response = generate(input_prompt, model=model, tokenizer=tokenizer, max_new_tokens=max_tokens)\n",
    "\n",
    "  class_id = extractor_func(response)\n",
    "\n",
    "  return (class_id, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xhhly4zjscN"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def predict_classes(dataset, model, tokenizer, classification_prompt, extractor_func, max_tokens):\n",
    "  \"\"\"\n",
    "    For a given RFPedia dataset, use the contents of each article to predict its label.\n",
    "\n",
    "    Args:\n",
    "      dataset (Dataset): The dataset to sample.\n",
    "      classifiction_prompt (str): Model instructions on how to classify articles.\n",
    "      extractor_func (func): A method which takes the model's response and returns a classification label as an integer.\n",
    "      max_tokens (int): Model response word limit.\n",
    "\n",
    "    Returns:\n",
    "      results (tuple<list, list, list>):\n",
    "        y_pred (list<int>): Predicted labels\n",
    "        y_true (list<int>): Actual labels (groundtruth)\n",
    "        responses (list<str>): Raw LLM response for each test sample.\n",
    "  \"\"\"\n",
    "  y_pred = []\n",
    "\n",
    "  labels = [message[-1]['content'] for message in dataset['messages']]\n",
    "  y_true = [extractor_func(label) for label in labels]\n",
    "    \n",
    "  responses = []\n",
    "\n",
    "  # TODO: This is unoptimized, use a dataset for this.\n",
    "  for item in tqdm(dataset, \"Classifying articles\"):\n",
    "\n",
    "    pred_label, response = get_category_label(item, model, tokenizer, classification_prompt, extractor_func, max_tokens)\n",
    "\n",
    "    y_pred.append( pred_label )\n",
    "    responses.append( response )\n",
    "\n",
    "  return y_pred, y_true, responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configure Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuned configuration\n",
    "configurations=[\n",
    "    {\"name\" : \"Fine-tuned\",\n",
    "     \"prompt\" : \"\",\n",
    "     \"max_tokens\" : 1,\n",
    "     \"extractor_func\" : get_class_label_from_string},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhwWWSqog7a7"
   },
   "outputs": [],
   "source": [
    "# Baseline configuration\n",
    "\n",
    "# configurations = [\n",
    "#     {\"name\" : \"Zero-shot\",\n",
    "#      \"prompt\" : PROMPT_ZEROSHOT,\n",
    "#      \"max_tokens\" : 10,\n",
    "#      \"extractor_func\" : get_first_number_from_string},\n",
    "\n",
    "#     {\"name\" : \"Chain-of-Thought\",\n",
    "#      \"prompt\" : PROMPT_COT,\n",
    "#      \"max_tokens\" : 100,\n",
    "#      \"extractor_func\" : get_class_label_from_string},\n",
    "\n",
    "#     {\"name\" : \"Meta Prompt\",\n",
    "#      \"prompt\" : PROMPT_META,\n",
    "#      \"max_tokens\" : 100,\n",
    "#      \"extractor_func\" : get_class_label_from_string},\n",
    "\n",
    "#     {\"name\" : \"2-Shot CoT\",\n",
    "#      \"prompt\" : PROMPT_COT_2SHOT,\n",
    "#      \"max_tokens\" : 100,\n",
    "#      \"extractor_func\" : get_class_label_from_string},\n",
    "\n",
    "#     {\"name\" : \"4-Shot CoT\",\n",
    "#      \"prompt\" : PROMPT_COT_4SHOT,\n",
    "#      \"max_tokens\" : 100,\n",
    "#      \"extractor_func\" : get_class_label_from_string}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJmzrHaLg3YL"
   },
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gx668V-G5ay"
   },
   "outputs": [],
   "source": [
    "# Predict all article categories in the dataset\n",
    "\n",
    "y_true = None\n",
    "\n",
    "for config in tqdm(configurations, \"Testing LLM configurations\"):\n",
    "\n",
    "  args = config['prompt'], config['extractor_func'], config['max_tokens']\n",
    "\n",
    "  config['y_pred'], config['y_true'], config['responses'] = predict_classes(ds['test'], model, tokenizer, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4Ib_XV9GKIr"
   },
   "source": [
    "## Return Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edIfaY5Qzm6c"
   },
   "source": [
    "### Save all LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers(config, class_labels, incorrect_only = False):\n",
    "    output_dir = os.path.join(\"output\", config['name'].replace(' ', '_').lower())\n",
    "    \n",
    "    # Get a boolean mask for every incorrect prediction\n",
    "    if incorrect_only:\n",
    "        mask = np.array(config['y_pred']) != np.array(config['y_true'])\n",
    "    else:\n",
    "        mask = np.full(len(config['y_pred']), True)\n",
    "    \n",
    "    # Get the index of every incorrect prediction\n",
    "    index = np.array(list(range(ds['test'].num_rows)))[mask]\n",
    "    \n",
    "    # Get every incorrect predicted label\n",
    "    labels = np.array(config['y_pred'])[mask]\n",
    "    \n",
    "    answers = {\n",
    "      # Obtain title and content of article\n",
    "      #\"Title\" : np.array([item['title'] for item in ds['test']])[mask],\n",
    "      \"Content\" : np.array([item['messages'][0]['content'] for item in ds['test']])[mask],\n",
    "    \n",
    "      # Get the class names for y_pred and y_true\n",
    "      \"Predicted Category\" : [class_labels[id] for id in np.array(config['y_pred'])[mask]],\n",
    "      \"Actual Category\" : np.array([item['messages'][-1]['content'] for item in ds['test']])[mask],\n",
    "    \n",
    "      # Get LLM raw text output\n",
    "      \"LLM Output\" : np.array(config['responses'])[mask]\n",
    "    }\n",
    "    \n",
    "    answers = pd.DataFrame(answers, index=index)\n",
    "    file_name = \"answers_incorrect.csv\" if incorrect_only else \"answers.csv\"\n",
    "    \n",
    "    answers.to_csv( os.path.join(output_dir, file_name), escapechar='\\\\' )\n",
    "    print(f\"\\n{config['name']} {\"incorrect \" if incorrect_only else \"\"}answers saved to: {os.path.join(output_dir, file_name)}\")\n",
    "    \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy report + Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmkxjb4_GLBN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def evaluate_model(config, class_labels, display_as_percentage = True):\n",
    "    \"\"\"\n",
    "    Evaluate a model configuration and save the results to output_dir.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = os.path.join(\"output\", config['name'].replace(' ', '_').lower())\n",
    "    shutil.rmtree(output_dir) # Remove any existing output files.\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output = {\n",
    "        \"name\": config[\"name\"],\n",
    "        \"prompt\" : config[\"prompt\"],\n",
    "        \"max_tokens\" : config[\"max_tokens\"],\n",
    "        \"data\" : [{\n",
    "            \"y_pred\" : config[\"y_pred\"],\n",
    "            \"y_true\" : config[\"y_true\"],\n",
    "            \"llm_output\" : config[\"responses\"]\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    with open( os.path.join(output_dir, \"output.json\"), \"w\" ) as f:\n",
    "        json.dump(output, f)\n",
    "    \n",
    "    y_true, y_pred, config_name = config['y_true'], config['y_pred'], config['name']\n",
    "    \n",
    "    # Get precision, recall, and F1 score\n",
    "    classif_report = classification_report(y_true, y_pred, zero_division=0.0, output_dict=True)\n",
    "    classif_report = pd.DataFrame(classif_report).transpose()\n",
    "    \n",
    "    # Save classification report to output dir\n",
    "    classif_report.to_csv( os.path.join(output_dir, \"evaluation.csv\"), escapechar='\\\\',index=False )\n",
    "\n",
    "    class_labels = class_labels[0:len(np.unique(y_pred))]\n",
    "\n",
    "    cm = confusion_matrix(y_true=y_true,y_pred=y_pred,normalize='true' if display_as_percentage else None)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_labels).plot(\n",
    "        cmap = plt.cm.Blues,\n",
    "        xticks_rotation='vertical',\n",
    "        text_kw={'fontsize': 6},\n",
    "        values_format='.0%' if display_as_percentage else None\n",
    "    )\n",
    "\n",
    "    plt.savefig( os.path.join(output_dir, \"confusion_matrix.png\"), dpi=200, bbox_inches='tight' )\n",
    "    print(f\"\\n{config_name} evaluation results saved to: {output_dir}\")\n",
    "    #plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URb1XhrrZbrF"
   },
   "outputs": [],
   "source": [
    "for config in configurations:\n",
    "    evaluate_model(config, class_labels=CLASS_LABELS)\n",
    "    save_answers(config, class_labels=CLASS_LABELS, incorrect_only = False)\n",
    "    save_answers(config, class_labels=CLASS_LABELS, incorrect_only = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
