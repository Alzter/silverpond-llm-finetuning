{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66057dd0-3155-4ca5-b7d3-4087416a4b99",
   "metadata": {},
   "source": [
    "# ▶️ Configure GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3604a738-c56d-4539-8813-d3506e872132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44078c-dcfc-487f-80af-56e2dbc04682",
   "metadata": {},
   "source": [
    "# ▶️ Load and Preprocess DBPedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902c19ec-61ce-415e-9251-4e396ba71ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for LLM finetuning and evaluation\n",
    "import finetune as ft\n",
    "import evaluate as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b64b75-1eda-4189-a2aa-eea6d1436ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"fancyzhx/dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd4f5a6-4ebe-468d-87ba-3be76dc32d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the dataset\n",
    "dataset = ft.sample_dataset(raw_dataset, labels_column=\"label\", ratio=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab91c64d-d6e7-49c2-a5d7-703856299210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset for fine-tuning\n",
    "dataset, label_names = ft.preprocess_dataset(dataset, text_column=\"content\", labels_column=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8fe57-8936-4b78-a75b-1c31b8cedc1e",
   "metadata": {},
   "source": [
    "# ▶️ Load Baseline LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b13ee8-e353-4f7d-b0d1-0da90ca1d26a",
   "metadata": {},
   "source": [
    "NOTE: I want to refactor this into one function -> ``ft.load_model(name, device_map, quantized)``\n",
    "\n",
    "I think it would be a bit nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81d26a1-5028-4f82-bfa4-f5ad3ebaed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MODEL_DEVICE = \"cuda:0\"\n",
    "QUANTIZED = True # Load model with 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721a682-05ab-47c9-81ad-c17c28596e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Same quantization configuration as QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.float16\n",
    ") if QUANTIZED else None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=MODEL_DEVICE,\n",
    "    use_cache=False # use_cache is incompatible with gradient checkpointing\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee3cf1-328f-4f9f-a3ad-7a0cd6fb057a",
   "metadata": {},
   "source": [
    "# ▶️ Evaluate Baseline LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaa34ff2-b1a4-45cc-a1b5-b34ef77d0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_prompts as prompts\n",
    "from evaluate import EvaluationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7490271f-82e2-4c33-abb0-5f1cb695436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the baseline LLM\n",
    "baseline_configurations = [\n",
    "    EvaluationConfig(\n",
    "        name=\"Zero-shot\",\n",
    "        prompt=prompts.DBPEDIA[\"ZERO_SHOT\"],\n",
    "        max_tokens=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f0f8e-3fe6-4aa6-88a2-abaf37506d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(configurations, model, tokenizer, label_names, eval_dataset):\n",
    "    results = []\n",
    "    for config in configurations:\n",
    "        result = ev.evaluate(\n",
    "            model=model, tokenizer=tokenizer, label_names=label_names,\n",
    "            eval_dataset=dataset['test'], eval_config=config\n",
    "        )\n",
    "        results.append(result)\n",
    "        result.save(\"output/dbpedia_14\") # Saves to \"output/dbpedia_14/<EvaluationConfig.name>\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e14f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = evaluate_model(baseline_configurations, model=model, tokenizer=tokenizer, label_names=label_names, eval_dataset=dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0eaf7f",
   "metadata": {},
   "source": [
    "# ▶️ Finetune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7396b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_LLM_NAME = \"models/Qwen2.5-FT-DBPedia\"\n",
    "\n",
    "LORA_RANK_DIMENSION = 6 # the rank of the adapter, the lower the fewer parameters you'll need to train. (smaller = more compression)\n",
    "LORA_ALPHA = 8 # this is the scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "LORA_DROPOUT = 0.05 # dropout probability for LoRA layers (helps prevent overfitting)\n",
    "MAX_SEQ_LENGTH = 64\n",
    "EPOCHS=1\n",
    "LEARNING_RATE=2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd13482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK_DIMENSION,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    auto_find_batch_size=True,\n",
    "    \n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=True,\n",
    "    \n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim='adamw_torch_fused',\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\", \n",
    "    \n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir=FINETUNED_LLM_NAME,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft.finetune( # Will save the model to the directory: FINETUNED_LLM_NAME\n",
    "#     model=model, tokenizer=tokenizer,\n",
    "#     train_dataset=dataset['train'],\n",
    "#     lora_config=lora_config, sft_config=sft_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ef836",
   "metadata": {},
   "source": [
    "# ▶️ Load Finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the baseline model if it exists, otherwise we will probably get an OOM exception\n",
    "import gc, torch\n",
    "\n",
    "if \"bnb_config\" in locals(): del bnb_config\n",
    "if \"tokenizer\" in locals(): del tokenizer\n",
    "if \"model\" in locals(): del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8909f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_LLM_NAME = \"models/Qwen2.5-FT-DBPedia\"\n",
    "MODEL_DEVICE = \"cuda:0\"\n",
    "QUANTIZED = True # Load model with 4-bit quantization\n",
    "\n",
    "model, tokenizer = ft.load_finetuned_llm(FINETUNED_LLM_NAME, MODEL_DEVICE, QUANTIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed475c01",
   "metadata": {},
   "source": [
    "# ▶️ Evaluate Finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_configurations = [\n",
    "    EvaluationConfig(\n",
    "        name=\"Fine-tuned\",\n",
    "        prompt=None,\n",
    "        max_tokens=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = evaluate_model(finetuned_configurations, model=model, tokenizer=tokenizer, label_names=label_names, eval_dataset=dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
