{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msss_data(msss_tables_dir : str, pattern = r\"AD_(.*).xls\") -> dict:\n",
    "    \"\"\"\n",
    "    Read all MSSS tables from a directory.\n",
    "\n",
    "    Args:\n",
    "        msss_tables_dir (str): Folder to search for MSSS table files from.\n",
    "        pattern (regexp, optional): Pattern to select MSSS table files and capture the label name. Defaults to r\"AD_(.*).xls\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of all MSSS data.\n",
    "    \"\"\"\n",
    "    msss_features = {}\n",
    "\n",
    "    for file in os.listdir(msss_tables_dir):\n",
    "\n",
    "        match = re.match(pattern=pattern, string=file)\n",
    "\n",
    "        if not match: continue\n",
    "\n",
    "        label_name = match.group(1)\n",
    "\n",
    "        path = os.path.join(msss_tables_dir, file)\n",
    "\n",
    "        df = pd.read_excel(path, header=1, index_col=\"Code\")\n",
    "\n",
    "        msss_features[label_name] = df[[\"Description\", \"Aux1\", \"Aux2\", \"Filter_Key\"]]\n",
    "\n",
    "    return msss_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_codes = load_msss_data(\"data/eq/datasets/ergon-ffa-msss-tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_DATA_PATH = \"data/eq/datasets/2024-tim-elkins-failures-data/\"\n",
    "\n",
    "EQ_DATA_FILES = [\n",
    "    \"EE Failures FY23-24 (Updated).xlsx\",\n",
    "    \"EGX Failures FY23-24 (updated).xlsx\"\n",
    "]\n",
    "\n",
    "EQ_DATA_PROVIDERS = [\"Ergon Energy\", \"Energex\"]\n",
    "\n",
    "EQ_DATA_FILES = [os.path.join(EQ_DATA_PATH, file) for file in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets\n",
    "data = [pd.read_excel(dataset) for dataset in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data backup\n",
    "# Executing this cell will restore the dataset to the original state\n",
    "\n",
    "from copy import copy\n",
    "if \"data2\" in locals():\n",
    "    data = data2\n",
    "data2 = copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"OUTAGE_ID\"\n",
    "\n",
    "input_features = [\n",
    "\"WEATHER_CONDITION\",\n",
    "\"OUTAGE_CAUSE\",\n",
    "\"FAULT_LONG_DESCRIPTION\",\n",
    "\"SHORT_DESC_2\",\n",
    "\"WORK_ORDER_COMPONENT_CODE_DESCRIPTION\",\n",
    "\"OUTAGE_CAUSE_GROUP\",\n",
    "\"OUTAGE_STANDARD_REASON_DESCRIPTION\",\n",
    "\"REASON_FOR_INTERRUPTION\",\n",
    "\"PROVIDER\" # We add this feature to the datasets during pre-processing\n",
    "]\n",
    "\n",
    "output_labels = [\n",
    "    \"MSSS_OBJECT\",\n",
    "    \"MSSS_DAMAGE\",\n",
    "    \"MSSS_CAUSE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize all column names and replace spaces with underscores\n",
    "for i, dataset in enumerate(data):\n",
    "    columns = dataset.columns.to_list()\n",
    "    new_columns = [column.replace(\" \", \"_\").upper() for column in columns]\n",
    "    replacement = dict(zip(columns, new_columns))\n",
    "    data[i] = dataset.rename(columns=replacement)\n",
    "\n",
    "for i, dataset in enumerate(data):\n",
    "    # Drop all rows with duplicate indices\n",
    "    dataset = dataset.drop_duplicates(subset=[index])\n",
    "\n",
    "    # Set OUTAGE_ID as the index field\n",
    "    dataset = dataset.set_index(index)\n",
    "\n",
    "    # Drop all non input/output columns\n",
    "    features = input_features + output_labels\n",
    "    features = [f for f in features if f in dataset.columns.to_list()]\n",
    "    dataset = dataset[features]\n",
    "    \n",
    "    # Drop all rows with entirely null/duplicate input values\n",
    "    features = [f for f in features if f in input_features]\n",
    "    dataset = dataset.drop_duplicates(subset=features).dropna(subset=features, how='all')\n",
    "    \n",
    "    # Add a feature to tell which dataset we're using (EE/EGX)\n",
    "    provider = EQ_DATA_PROVIDERS[i]\n",
    "    dataset[\"PROVIDER\"] = provider\n",
    "\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the index for each dataset\n",
    "for i, dataset in enumerate(data):\n",
    "    dataset = dataset.reset_index(drop=False)\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all datasets into one\n",
    "dataset = pd.concat(data)\n",
    "dataset.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed.csv\")\n",
    "\n",
    "dataset.to_csv(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate each sample's input features into strings\n",
    "\n",
    "# # Get all input features present in the dataset\n",
    "# inputs = [f for f in input_features if f in dataset.columns]\n",
    "\n",
    "# # Replace all missing input feature text with \"Unknown\"\n",
    "# dataset[inputs] = dataset[inputs].fillna(\"Unknown\")\n",
    "\n",
    "# # Concatenate each sample's input features into strings\n",
    "# input_texts = []\n",
    "# for _, sample in dataset[inputs].iterrows():\n",
    "\n",
    "#     text = [f\"{key}: {value}\" for key, value in sample.items()]\n",
    "#     text = \";\\n\".join(text)\n",
    "#     input_texts.append(text)\n",
    "\n",
    "# # Add all concatenated inputs as a new feature\n",
    "# dataset[\"TEXT\"] = input_texts\n",
    "\n",
    "# # Delete individual input features\n",
    "# dataset = dataset.drop(columns=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed.csv\")\n",
    "\n",
    "# dataset.to_csv(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"../data/eq/datasets/preprocessed.csv\", index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
