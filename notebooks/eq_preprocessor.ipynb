{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msss_tables(msss_tables_dir : str) -> dict:\n",
    "    MSSS_TABLES = [\n",
    "        \"AD_Outage_Type.xls\",\n",
    "        \"AD_Componant.xls\",\n",
    "        \"AD_Object.xls\",\n",
    "        \"AD_Damage.xls\",\n",
    "        \"AD_Cause.xls\"\n",
    "    ]\n",
    "\n",
    "    msss_tables = []\n",
    "\n",
    "    for level, table in enumerate(MSSS_TABLES):\n",
    "        path = os.path.join(msss_tables_dir, table)\n",
    "        data = pd.read_excel(path, header=1, index_col=\"Code\")\n",
    "        data = data.dropna(axis=1, how='all') # Drop null rows\n",
    "\n",
    "        # Get the column which contains the MSSS Code\n",
    "        msss_code_column = \"Aux2\" if \"Filter_Key\" in data else \"Aux1\"\n",
    "        # Rename this column to \"ID\"\n",
    "        data = data.rename(columns={\"Description\": \"Name\", msss_code_column: \"ID\"})\n",
    "\n",
    "        # If \"ID\" column only has two states, convert it into a boolean\n",
    "        if data[\"ID\"].nunique() == 1:\n",
    "            value = data[\"ID\"].unique()[0]\n",
    "            data[\"ID\"] = data[\"ID\"].map(lambda x: x == value)\n",
    "\n",
    "        # Get the column which contains the previous MSSS code\n",
    "        if \"Filter_Key\" in data:\n",
    "            # Rename this column to \"PrevID\"\n",
    "            data = data.rename(columns={\"Filter_Key\":\"PrevID\"})\n",
    "        else:\n",
    "            # If none exists, create an empty one\n",
    "            data.insert(data.columns.size, \"PrevID\", np.nan)\n",
    "\n",
    "        data = data[[\"Name\", \"ID\", \"PrevID\"]]\n",
    "        msss_tables.append(data)\n",
    "\n",
    "    return msss_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_codes(table : pd.DataFrame, prev_codes : list | None = None, prev_table : pd.DataFrame | None = None) -> list:\n",
    "    \"\"\"This function generates a list of MSSS codes given a MSSS table.\n",
    "    It can be used recursively to generate every unique combination of MSSS codes in the MSSS taxonomy.\n",
    "\n",
    "    Args:\n",
    "        table (DataFrame): A MSSS table from load_msss_tables().\n",
    "        prev_codes (list, optional): List of the MSSS codes from the previous table.\n",
    "        prev_table (pd.DataFrame, optional): The previous MSSS table. Used to translate MSSS codes into indices.\n",
    "\n",
    "    Returns:\n",
    "        codes (list): A list of the table's MSSS codes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not prev_codes:\n",
    "        # If the table does not have a previous MSSS Code column,\n",
    "        # we can simply use the unique MSSS Codes from the table\n",
    "        if \"PrevID\" not in table.dropna(axis=1,how='any'):\n",
    "            codes = table[\"ID\"].unique().tolist()\n",
    "            return codes\n",
    "        else:\n",
    "            raise TypeError(\"Getting MSSS codes for this table requires the codes from the previous table!\")\n",
    "    \n",
    "    codes = prev_codes\n",
    "\n",
    "    # Use the MSSS Code column as the category\n",
    "    if not pd.api.types.is_bool_dtype(table[\"ID\"]):\n",
    "        category_column = \"ID\"\n",
    "\n",
    "    # Unless the column is a boolean, in which case use the numerical code\n",
    "    else:\n",
    "        table = table.reset_index()\n",
    "        category_column = \"Code\"\n",
    "\n",
    "    # For each category in the previous MSSS table\n",
    "    for i, code in enumerate(prev_codes):\n",
    "\n",
    "        last_code = code[-1] if type(code) is list else code\n",
    "\n",
    "        # Get all sub-categories in this MSSS table\n",
    "        subcodes = table[table[\"PrevID\"] == last_code][category_column].unique().tolist()\n",
    "        \n",
    "        # Convert the last code entry from MSSS Code -> numerical code\n",
    "        if prev_table is not None:\n",
    "            last_code_index = prev_table[prev_table[\"ID\"] == last_code].index[0]\n",
    "\n",
    "            if type(code) is list:\n",
    "                code[-1] = last_code_index\n",
    "            else: code = last_code_index\n",
    "\n",
    "        if type(code) is list:\n",
    "            subcodes = [[*code, subcode] for subcode in subcodes]\n",
    "        else:\n",
    "            subcodes = [[code, subcode] for subcode in subcodes]\n",
    "        \n",
    "        codes[i] = subcodes\n",
    "\n",
    "    # Flatten the list\n",
    "    codes = [i for j in codes for i in j]\n",
    "\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_msss_lookup_table(tables : list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a lookup table with a row for every unique MSSS classification.\n",
    "\n",
    "    Args:\n",
    "        tables (list[DataFrame]): List of pre-processed MSSS tables obtained with load_msss_tables().\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The MSSS lookup table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of lists for every possible MSSS classification\n",
    "    codes = None\n",
    "    for table_id, table in enumerate(tables):\n",
    "        codes = _get_codes(table, codes, tables[table_id - 1] if table_id > 0 else None)\n",
    "    \n",
    "    # Create a DataFrame to represent all the MSSS classifications\n",
    "    msss_features = [\"Outage Type\", \"Component\", \"Object\", \"Damage\", \"Cause\"]\n",
    "    index = pd.DataFrame(codes, columns=msss_features)\n",
    "\n",
    "    # Add features in the index for the Description and MSSS Code for each feature\n",
    "    for column, table in zip(msss_features, tables):\n",
    "        index[f\"{column} Description\"] = index[column].map(lambda x: table[\"Name\"][x])\n",
    "        index[f\"{column} Code\"] = index[column].map(lambda x: table[\"ID\"][x])\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_tables_path = \"../data/eq/datasets/ergon-ffa-msss-tables\"\n",
    "tables = load_msss_tables(msss_tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_table = create_msss_lookup_table(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_table.to_csv(\"data/eq/datasets/msss_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classif(msss_object_description : str, msss_damage_description : str, msss_cause_description : str, lookup_table : pd.DataFrame) -> pd.DataFrame:\n",
    "    t = lookup_table\n",
    "    return t.loc[\n",
    "        (t[\"Object Description\"] == msss_object_description) & \n",
    "        (t[\"Damage Description\"] == msss_damage_description) &\n",
    "        (t[\"Cause Description\"] == msss_cause_description)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_DATA_PATH = \"../data/eq/datasets/2024-tim-elkins-failures-data/\"\n",
    "\n",
    "EQ_DATA_FILES = [\n",
    "    \"EE Failures FY23-24 (Updated).xlsx\",\n",
    "    \"EGX Failures FY23-24 (updated).xlsx\"\n",
    "]\n",
    "\n",
    "EQ_DATA_PROVIDERS = [\"Ergon Energy\", \"Energex\"]\n",
    "\n",
    "EQ_DATA_FILES = [os.path.join(EQ_DATA_PATH, file) for file in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets\n",
    "data = [pd.read_excel(dataset) for dataset in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"OUTAGE_ID\"\n",
    "\n",
    "input_features = [\n",
    "\"WEATHER_CONDITION\",\n",
    "\"OUTAGE_CAUSE\",\n",
    "\"FAULT_LONG_DESCRIPTION\",\n",
    "\"SHORT_DESC_2\",\n",
    "\"WORK_ORDER_COMPONENT_CODE_DESCRIPTION\",\n",
    "\"OUTAGE_CAUSE_GROUP\",\n",
    "\"OUTAGE_STANDARD_REASON_DESCRIPTION\",\n",
    "\"REASON_FOR_INTERRUPTION\",\n",
    "\"PROVIDER\" # We add this feature to the datasets during pre-processing\n",
    "]\n",
    "\n",
    "output_labels = [\n",
    "    \"MSSS_OBJECT_DESCRIPTION\",\n",
    "    \"MSSS_DAMAGE_DESCRIPTION\",\n",
    "    \"MSSS_CAUSE_DESCRIPTION\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data backup\n",
    "# Executing this cell will restore the dataset to the original state\n",
    "\n",
    "from copy import copy\n",
    "if \"data2\" in locals():\n",
    "    data = data2\n",
    "data2 = copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize all column names and replace spaces with underscores\n",
    "for i, dataset in enumerate(data):\n",
    "    columns = dataset.columns.to_list()\n",
    "    new_columns = [column.replace(\" \", \"_\").upper() for column in columns]\n",
    "    replacement = dict(zip(columns, new_columns))\n",
    "    data[i] = dataset.rename(columns=replacement)\n",
    "\n",
    "for i, dataset in enumerate(data):\n",
    "    # Drop all non input/output columns\n",
    "    features = input_features + output_labels + [index]\n",
    "    features = [f for f in features if f in dataset.columns]\n",
    "    dataset = dataset[features]\n",
    "    \n",
    "    # Drop all rows with entirely null input features\n",
    "    features = [f for f in features if f in input_features]\n",
    "    dataset = dataset.dropna(subset=features, how='all')\n",
    "    \n",
    "    # Add a feature to tell which dataset we're using (EE/EGX)\n",
    "    provider = EQ_DATA_PROVIDERS[i]\n",
    "    dataset.insert(dataset.columns.size, \"PROVIDER\", provider)\n",
    "\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain null-like string values should be replaced with null\n",
    "for i, dataset in enumerate(data):\n",
    "    null_like = [\"<null>\", \"#REF!\"]\n",
    "\n",
    "    for j in null_like: dataset = dataset.replace(j, np.nan)\n",
    "    \n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with duplicate outage ID and output features\n",
    "for i, dataset in enumerate(data):\n",
    "    dataset = dataset.drop_duplicates(subset=[index, *output_labels], keep='last')\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all datasets into one\n",
    "dataset = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make spacing / capitalisation of MSSS feature labels consistent\n",
    "dataset[output_labels] = dataset[output_labels].map(lambda x: x.strip().title() if type(x) is str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct inconsistent capitalisation within MSSS feature labels where possible\n",
    "\n",
    "def map_item_to_msss_feature_label(item : str, label_names : list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Given a MSSS label (e.g., \" animal\"), try to map\n",
    "    it to an MSSS label from a list (e.g., \"Animal\")\n",
    "    to correct issues in capitalisation / spacing.\n",
    "\n",
    "    Args:\n",
    "        item (str): The raw label name.\n",
    "        label_names (list[str]): List of legal label names.\n",
    "\n",
    "    Returns:\n",
    "        label (str | None): The correct label name, if found, otherwise the raw label name.\n",
    "    \"\"\"\n",
    "    if pd.isna(item): return np.nan\n",
    "\n",
    "    if item in label_names: return item\n",
    "\n",
    "    original_item = item\n",
    "    item = item.strip().lower()\n",
    "\n",
    "    labels_easy = [i.strip().lower() for i in label_names]\n",
    "    if item in labels_easy:\n",
    "        index = labels_easy.index(item)\n",
    "        return label_names[ index ]\n",
    "\n",
    "    word_matches = [i for i in labels_easy if item in i]\n",
    "    \n",
    "    if word_matches:\n",
    "        if len(word_matches) == 1:\n",
    "            item = word_matches[0]\n",
    "            index = labels_easy.index(item)\n",
    "            return label_names[ index ]\n",
    "    \n",
    "    return original_item\n",
    "\n",
    "def rectify_feature(dataset : pd.DataFrame, msss_feature : str, msss_table : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Correct an MSSS feature from a dataset by removing inconsistencies in capitalisation / spacing.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The original dataset\n",
    "        msss_feature (str): MSSS feature name, e.g., \"Cause\".\n",
    "        msss_table (pd.DataFrame): MSSS lookup table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with msss_feature rectified.\n",
    "    \"\"\"\n",
    "    table_column = f\"{msss_feature} Description\"\n",
    "    data_column = f\"MSSS_{msss_feature.upper()}_DESCRIPTION\"\n",
    "\n",
    "    label_names = msss_table[table_column].unique().tolist()\n",
    "\n",
    "    dataset[data_column] = dataset[data_column].map(lambda x: map_item_to_msss_feature_label(x, label_names))\n",
    "    return dataset\n",
    "\n",
    "for feature in [\"Object\", \"Cause\", \"Damage\"]:\n",
    "    dataset = rectify_feature(dataset, feature, msss_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Eval Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data in a deterministic way\n",
    "dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataset.dropna(subset=output_labels, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised dataset\n",
    "data_train = dataset.dropna(subset=output_labels, how='any')\n",
    "\n",
    "# Unsupervised dataset\n",
    "data_eval = dataset.drop(data_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce training data class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "data_resampled = copy(data_train)\n",
    "size = len(data_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all classes with less than 50 samples\n",
    "min_class_size = 50\n",
    "while True:\n",
    "    for label in output_labels:\n",
    "        class_counts = data_resampled[label].value_counts()\n",
    "        classes_to_remove = class_counts.where(lambda x: x <= min_class_size).dropna().keys().to_list()\n",
    "        data_resampled = data_resampled[~data_resampled[label].isin(classes_to_remove)]\n",
    "\n",
    "    if [data_resampled[label].value_counts().where(lambda x: x <= min_class_size).dropna().size for label in output_labels] == [0] * len(output_labels):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample minority classes to improve class distribution\n",
    "for label in output_labels:\n",
    "    features = input_features + [i for i in output_labels if not i == label]\n",
    "\n",
    "    X, y = data_resampled[features], data_resampled[label]\n",
    "    \n",
    "    X, y = ros.fit_resample(X, y)\n",
    "\n",
    "    data_resampled = pd.concat([X, y], axis=1)\n",
    "    data_resampled = data_resampled.dropna()\n",
    "    data_resampled = data_resampled.sample(n=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old VS new class distributions\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for i, label in enumerate(output_labels):\n",
    "    ax = plt.subplot(2,3,i + 1)\n",
    "    plt.plot(data_train[label].value_counts())\n",
    "    plt.xticks([])\n",
    "    #plt.ylim([0,10000])\n",
    "    plt.title(label.rstrip(\"DESCRIPTION\").lstrip(\"MSSS\"))\n",
    "    ax.set_yscale('log')\n",
    "    plt.ylim([1, 100000])\n",
    "\n",
    "plt.suptitle(\"MSSS class distribution\")\n",
    "for i, label in enumerate(output_labels):\n",
    "    ax = plt.subplot(2,3,i + 4)\n",
    "    plt.plot(data_resampled[label].value_counts())\n",
    "    plt.xticks([])\n",
    "    #plt.ylim([0,10000])\n",
    "    plt.title(label.rstrip(\"DESCRIPTION\").lstrip(\"MSSS\"))\n",
    "    ax.set_yscale('log')\n",
    "    plt.ylim([1, 100000])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv(\n",
    "    os.path.join(EQ_DATA_PATH, \"../data_train.csv\"),\n",
    "    index=False)\n",
    "\n",
    "data_eval.to_csv(\n",
    "    os.path.join(EQ_DATA_PATH, \"../data_eval.csv\"),\n",
    "    index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
