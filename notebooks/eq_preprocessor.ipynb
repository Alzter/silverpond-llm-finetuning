{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msss_data(msss_tables_dir : str, pattern = r\"AD_(.*).xls\") -> dict:\n",
    "    \"\"\"\n",
    "    Read all MSSS tables from a directory.\n",
    "\n",
    "    Args:\n",
    "        msss_tables_dir (str): Folder to search for MSSS table files from.\n",
    "        pattern (regexp, optional): Pattern to select MSSS table files and capture the label name. Defaults to r\"AD_(.*).xls\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of all MSSS data.\n",
    "    \"\"\"\n",
    "    msss_features = {}\n",
    "\n",
    "    for file in os.listdir(msss_tables_dir):\n",
    "\n",
    "        match = re.match(pattern=pattern, string=file)\n",
    "\n",
    "        if not match: continue\n",
    "\n",
    "        label_name = match.group(1)\n",
    "\n",
    "        path = os.path.join(msss_tables_dir, file)\n",
    "\n",
    "        df = pd.read_excel(path, header=1, index_col=\"Code\")\n",
    "\n",
    "        msss_features[label_name] = df[[\"Description\", \"Aux1\", \"Aux2\", \"Filter_Key\"]]\n",
    "\n",
    "    return msss_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_codes = load_msss_data(\"data/eq/datasets/ergon-ffa-msss-tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_DATA_PATH = \"data/eq/datasets/2024-tim-elkins-failures-data/\"\n",
    "\n",
    "EQ_DATA_FILES = [\n",
    "    \"EE Failures FY23-24 (Updated).xlsx\",\n",
    "    \"EGX Failures FY23-24 (updated).xlsx\"\n",
    "]\n",
    "\n",
    "EQ_DATA_PROVIDERS = [\"Ergon Energy\", \"Energex\"]\n",
    "\n",
    "EQ_DATA_FILES = [os.path.join(EQ_DATA_PATH, file) for file in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets\n",
    "data = [pd.read_excel(dataset) for dataset in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"OUTAGE_ID\"\n",
    "\n",
    "input_features = [\n",
    "\"WEATHER_CONDITION\",\n",
    "\"OUTAGE_CAUSE\",\n",
    "\"FAULT_LONG_DESCRIPTION\",\n",
    "\"SHORT_DESC_2\",\n",
    "\"WORK_ORDER_COMPONENT_CODE_DESCRIPTION\",\n",
    "\"OUTAGE_CAUSE_GROUP\",\n",
    "\"OUTAGE_STANDARD_REASON_DESCRIPTION\",\n",
    "\"REASON_FOR_INTERRUPTION\",\n",
    "\"PROVIDER\" # We add this feature to the datasets during pre-processing\n",
    "]\n",
    "\n",
    "output_labels = [\n",
    "    \"MSSS_OBJECT_DESCRIPTION\",\n",
    "    \"MSSS_DAMAGE_DESCRIPTION\",\n",
    "    \"MSSS_CAUSE_DESCRIPTION\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data backup\n",
    "# Executing this cell will restore the dataset to the original state\n",
    "\n",
    "from copy import copy\n",
    "if \"data2\" in locals():\n",
    "    data = data2\n",
    "data2 = copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize all column names and replace spaces with underscores\n",
    "for i, dataset in enumerate(data):\n",
    "    columns = dataset.columns.to_list()\n",
    "    new_columns = [column.replace(\" \", \"_\").upper() for column in columns]\n",
    "    replacement = dict(zip(columns, new_columns))\n",
    "    data[i] = dataset.rename(columns=replacement)\n",
    "\n",
    "for i, dataset in enumerate(data):\n",
    "    # Drop all rows with duplicate indices\n",
    "    # dataset = dataset.drop_duplicates(subset=[index])\n",
    "\n",
    "    # Set OUTAGE_ID as the index field\n",
    "    # dataset = dataset.set_index(index)\n",
    "\n",
    "    # Drop all non input/output columns\n",
    "    features = input_features + output_labels + [index]\n",
    "    features = [f for f in features if f in dataset.columns]\n",
    "    dataset = dataset[features]\n",
    "    \n",
    "    # Drop all rows with entirely null input values\n",
    "    features = [f for f in features if f in input_features]\n",
    "    dataset = dataset.dropna(subset=features, how='all')\n",
    "    \n",
    "    # Add a feature to tell which dataset we're using (EE/EGX)\n",
    "    provider = EQ_DATA_PROVIDERS[i]\n",
    "    dataset.insert(dataset.columns.size, \"PROVIDER\", provider)\n",
    "\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(rows : pd.Series) -> object:\n",
    "    \"\"\"\n",
    "    Combine a series of rows into one item.\n",
    "\n",
    "    The combination uses different methods based on the data type of the rows:\n",
    "    - ``str``: Returns the longest string.\n",
    "    - ``int``: Returns the mode.\n",
    "    - ``nan``: Returns ``nan``.\n",
    "\n",
    "    Args:\n",
    "        rows (pd.Series): The series of rows.\n",
    "\n",
    "    Returns:\n",
    "        object: The combination of the rows.\n",
    "    \"\"\"\n",
    "    # If the series is entirely empty, return NaN\n",
    "    if rows.value_counts().empty: return np.nan\n",
    "\n",
    "    # If the series is made of strings:\n",
    "    if rows.dtype == \"object\":\n",
    "        # Return the longest string in the series\n",
    "        return rows.loc[rows.str.len().idxmax()]\n",
    "    else:\n",
    "        # Otherwise, return the mode\n",
    "        return rows.value_counts().index[0]\n",
    "\n",
    "# Combine all rows which have duplicate outage IDs\n",
    "for i, dataset in enumerate(data):\n",
    "    dataset = dataset.groupby(index).agg(combine).reset_index()\n",
    "\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data backup\n",
    "# Executing this cell will restore the dataset to the original state\n",
    "\n",
    "from copy import copy\n",
    "if \"data3\" in locals():\n",
    "    data = data3\n",
    "data3 = copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicate rows which have different labels (inconsistent rows)\n",
    "# Remove all rows with duplicate input features\n",
    "for i, dataset in enumerate(data):\n",
    "    features = [f for f in input_features if f in dataset.columns]\n",
    "    rows_to_remove = []\n",
    "\n",
    "    # For all rows with identical input features:\n",
    "    for _, rows in dataset.groupby(features):\n",
    "        if len(rows) == 1: continue\n",
    "\n",
    "        # Determine whether the output labels of these rows are identical\n",
    "        consistent = rows.duplicated(subset=output_labels, keep=False).all()\n",
    "        \n",
    "        # If the rows are consistent, only remove the duplicate entries.\n",
    "        # If not, remove all entries.\n",
    "        indices = rows.index\n",
    "        if consistent: indices = indices[1:]\n",
    "\n",
    "        rows_to_remove.extend(indices)\n",
    "        \n",
    "    dataset = dataset.drop(rows_to_remove)\n",
    "\n",
    "    # TODO: This step shouldn't be necessary,\n",
    "    # but for some reason not all duplicate rows\n",
    "    # get removed in the previous step. How to fix?\n",
    "    #dataset = dataset.drop_duplicates(subset=features, keep='first')\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dupes = data[1][ data[1].duplicated(keep=False) ]\n",
    "\n",
    "# # Obtain a 2D list containing all rows in the dataset with duplicate indices grouped by index.\n",
    "# # Source: https://stackoverflow.com/a/46629549\n",
    "# duplicate_indices = dupes.groupby(index).apply(lambda x : list(x.index), include_groups=False).tolist()\n",
    "\n",
    "# for indices in duplicate_indices:\n",
    "\n",
    "#     # For all rows which have the same Outage ID:\n",
    "#     rows = data[1].iloc[indices]\n",
    "\n",
    "#     # Combine all columns in the rows into one.\n",
    "#     for column in data[1].columns:\n",
    "        \n",
    "#         values = rows[column]\n",
    "\n",
    "#         combined = values.dropna().mode()[0]\n",
    "\n",
    "#         # Replace the first index with the combined rows\n",
    "#         first_index = indices[0]\n",
    "#         #data[1].loc[index, column] = combined\n",
    "#         data[1][column][first_index] = combined\n",
    "\n",
    "# data[1] = data[1].drop_duplicates(subset=index,keep='first')\n",
    "# #data[1] = data[1].drop(rows_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the index for each dataset\n",
    "# for i, dataset in enumerate(data):\n",
    "#     dataset = dataset.reset_index(drop=False)\n",
    "#     data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all datasets into one\n",
    "dataset = pd.concat(data)\n",
    "dataset = dataset.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed.csv\")\n",
    "\n",
    "dataset.to_csv(new_path)\n",
    "\n",
    "new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed-supervised.csv\")\n",
    "\n",
    "dataset.dropna(subset=output_labels).to_csv(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate each sample's input features into strings\n",
    "\n",
    "# # Get all input features present in the dataset\n",
    "# inputs = [f for f in input_features if f in dataset.columns]\n",
    "\n",
    "# # Replace all missing input feature text with \"Unknown\"\n",
    "# dataset[inputs] = dataset[inputs].fillna(\"Unknown\")\n",
    "\n",
    "# # Concatenate each sample's input features into strings\n",
    "# input_texts = []\n",
    "# for _, sample in dataset[inputs].iterrows():\n",
    "\n",
    "#     text = [f\"{key}: {value}\" for key, value in sample.items()]\n",
    "#     text = \";\\n\".join(text)\n",
    "#     input_texts.append(text)\n",
    "\n",
    "# # Add all concatenated inputs as a new feature\n",
    "# dataset[\"TEXT\"] = input_texts\n",
    "\n",
    "# # Delete individual input features\n",
    "# dataset = dataset.drop(columns=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed.csv\")\n",
    "\n",
    "# dataset.to_csv(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"../data/eq/datasets/preprocessed.csv\", index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
