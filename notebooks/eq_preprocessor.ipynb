{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msss_tables(msss_tables_dir : str) -> dict:\n",
    "    MSSS_TABLES = [\n",
    "        \"AD_Outage_Type.xls\",\n",
    "        \"AD_Componant.xls\",\n",
    "        \"AD_Object.xls\",\n",
    "        \"AD_Damage.xls\",\n",
    "        \"AD_Cause.xls\"\n",
    "    ]\n",
    "\n",
    "    msss_tables = []\n",
    "\n",
    "    for level, table in enumerate(MSSS_TABLES):\n",
    "        path = os.path.join(msss_tables_dir, table)\n",
    "        data = pd.read_excel(path, header=1, index_col=\"Code\")\n",
    "        data = data.dropna(axis=1, how='all') # Drop null rows\n",
    "\n",
    "        # Get the column which contains the MSSS Code\n",
    "        msss_code_column = \"Aux2\" if \"Filter_Key\" in data else \"Aux1\"\n",
    "        # Rename this column to \"ID\"\n",
    "        data = data.rename(columns={\"Description\": \"Name\", msss_code_column: \"ID\"})\n",
    "\n",
    "        # If \"ID\" column only has two states, convert it into a boolean\n",
    "        if data[\"ID\"].nunique() == 1:\n",
    "            value = data[\"ID\"].unique()[0]\n",
    "            data[\"ID\"] = data[\"ID\"].map(lambda x: x == value)\n",
    "\n",
    "        # Get the column which contains the previous MSSS code\n",
    "        if \"Filter_Key\" in data:\n",
    "            # Rename this column to \"PrevID\"\n",
    "            data = data.rename(columns={\"Filter_Key\":\"PrevID\"})\n",
    "        else:\n",
    "            # If none exists, create an empty one\n",
    "            data.insert(data.columns.size, \"PrevID\", np.nan)\n",
    "\n",
    "        data = data[[\"Name\", \"ID\", \"PrevID\"]]\n",
    "        msss_tables.append(data)\n",
    "\n",
    "    return msss_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_codes(table : pd.DataFrame, prev_codes : list | None = None, prev_table : pd.DataFrame | None = None) -> list:\n",
    "    \"\"\"This function generates a list of MSSS codes given a MSSS table.\n",
    "    It can be used recursively to generate every unique combination of MSSS codes in the MSSS taxonomy.\n",
    "\n",
    "    Args:\n",
    "        table (DataFrame): A MSSS table from load_msss_tables().\n",
    "        prev_codes (list, optional): List of the MSSS codes from the previous table.\n",
    "        prev_table (pd.DataFrame, optional): The previous MSSS table. Used to translate MSSS codes into indices.\n",
    "\n",
    "    Returns:\n",
    "        codes (list): A list of the table's MSSS codes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not prev_codes:\n",
    "        # If the table does not have a previous MSSS Code column,\n",
    "        # we can simply use the unique MSSS Codes from the table\n",
    "        if \"PrevID\" not in table.dropna(axis=1,how='any'):\n",
    "            codes = table[\"ID\"].unique().tolist()\n",
    "            return codes\n",
    "        else:\n",
    "            raise TypeError(\"Getting MSSS codes for this table requires the codes from the previous table!\")\n",
    "    \n",
    "    codes = prev_codes\n",
    "\n",
    "    # Use the MSSS Code column as the category\n",
    "    if not pd.api.types.is_bool_dtype(table[\"ID\"]):\n",
    "        category_column = \"ID\"\n",
    "\n",
    "    # Unless the column is a boolean, in which case use the numerical code\n",
    "    else:\n",
    "        table = table.reset_index()\n",
    "        category_column = \"Code\"\n",
    "\n",
    "    # For each category in the previous MSSS table\n",
    "    for i, code in enumerate(prev_codes):\n",
    "\n",
    "        last_code = code[-1] if type(code) is list else code\n",
    "\n",
    "        # Get all sub-categories in this MSSS table\n",
    "        subcodes = table[table[\"PrevID\"] == last_code][category_column].unique().tolist()\n",
    "        \n",
    "        # Convert the last code entry from MSSS Code -> numerical code\n",
    "        if prev_table is not None:\n",
    "            last_code_index = prev_table[prev_table[\"ID\"] == last_code].index[0]\n",
    "\n",
    "            if type(code) is list:\n",
    "                code[-1] = last_code_index\n",
    "            else: code = last_code_index\n",
    "\n",
    "        if type(code) is list:\n",
    "            subcodes = [[*code, subcode] for subcode in subcodes]\n",
    "        else:\n",
    "            subcodes = [[code, subcode] for subcode in subcodes]\n",
    "        \n",
    "        codes[i] = subcodes\n",
    "\n",
    "    # Flatten the list\n",
    "    codes = [i for j in codes for i in j]\n",
    "\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_msss_lookup_table(tables : list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a lookup table with a row for every unique MSSS classification.\n",
    "\n",
    "    Args:\n",
    "        tables (list[DataFrame]): List of pre-processed MSSS tables obtained with load_msss_tables().\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The MSSS lookup table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of lists for every possible MSSS classification\n",
    "    codes = None\n",
    "    for table_id, table in enumerate(tables):\n",
    "        codes = _get_codes(table, codes, tables[table_id - 1] if table_id > 0 else None)\n",
    "    \n",
    "    # Create a DataFrame to represent all the MSSS classifications\n",
    "    msss_features = [\"Outage Type\", \"Component\", \"Object\", \"Damage\", \"Cause\"]\n",
    "    index = pd.DataFrame(codes, columns=msss_features)\n",
    "\n",
    "    # Add features in the index for the Description and MSSS Code for each feature\n",
    "    for column, table in zip(msss_features, tables):\n",
    "        index[f\"{column} Description\"] = index[column].map(lambda x: table[\"Name\"][x])\n",
    "        index[f\"{column} Code\"] = index[column].map(lambda x: table[\"ID\"][x])\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_tables_path = \"data/eq/datasets/ergon-ffa-msss-tables\"\n",
    "tables = load_msss_tables(msss_tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_table = create_msss_lookup_table(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "msss_table.to_csv(\"data/eq/datasets/msss_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classif(msss_object_description : str, msss_damage_description : str, msss_cause_description : str, lookup_table : pd.DataFrame) -> pd.DataFrame:\n",
    "    t = lookup_table\n",
    "    return t.loc[\n",
    "        (t[\"Object Description\"] == msss_object_description) & \n",
    "        (t[\"Damage Description\"] == msss_damage_description) &\n",
    "        (t[\"Cause Description\"] == msss_cause_description)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_DATA_PATH = \"data/eq/datasets/2024-tim-elkins-failures-data/\"\n",
    "\n",
    "EQ_DATA_FILES = [\n",
    "    \"EE Failures FY23-24 (Updated).xlsx\",\n",
    "    \"EGX Failures FY23-24 (updated).xlsx\"\n",
    "]\n",
    "\n",
    "EQ_DATA_PROVIDERS = [\"Ergon Energy\", \"Energex\"]\n",
    "\n",
    "EQ_DATA_FILES = [os.path.join(EQ_DATA_PATH, file) for file in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the datasets\n",
    "data = [pd.read_excel(dataset) for dataset in EQ_DATA_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"OUTAGE_ID\"\n",
    "\n",
    "input_features = [\n",
    "\"WEATHER_CONDITION\",\n",
    "\"OUTAGE_CAUSE\",\n",
    "\"FAULT_LONG_DESCRIPTION\",\n",
    "\"SHORT_DESC_2\",\n",
    "\"WORK_ORDER_COMPONENT_CODE_DESCRIPTION\",\n",
    "\"OUTAGE_CAUSE_GROUP\",\n",
    "\"OUTAGE_STANDARD_REASON_DESCRIPTION\",\n",
    "\"REASON_FOR_INTERRUPTION\",\n",
    "\"PROVIDER\" # We add this feature to the datasets during pre-processing\n",
    "]\n",
    "\n",
    "output_labels = [\n",
    "    \"MSSS_OBJECT_DESCRIPTION\",\n",
    "    \"MSSS_DAMAGE_DESCRIPTION\",\n",
    "    \"MSSS_CAUSE_DESCRIPTION\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data backup\n",
    "# Executing this cell will restore the dataset to the original state\n",
    "\n",
    "from copy import copy\n",
    "if \"data2\" in locals():\n",
    "    data = data2\n",
    "data2 = copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize all column names and replace spaces with underscores\n",
    "for i, dataset in enumerate(data):\n",
    "    columns = dataset.columns.to_list()\n",
    "    new_columns = [column.replace(\" \", \"_\").upper() for column in columns]\n",
    "    replacement = dict(zip(columns, new_columns))\n",
    "    data[i] = dataset.rename(columns=replacement)\n",
    "\n",
    "for i, dataset in enumerate(data):\n",
    "    # Drop all non input/output columns\n",
    "    features = input_features + output_labels + [index]\n",
    "    features = [f for f in features if f in dataset.columns]\n",
    "    dataset = dataset[features]\n",
    "    \n",
    "    # Drop all rows with entirely null input features\n",
    "    features = [f for f in features if f in input_features]\n",
    "    dataset = dataset.dropna(subset=features, how='all')\n",
    "    \n",
    "    # Add a feature to tell which dataset we're using (EE/EGX)\n",
    "    provider = EQ_DATA_PROVIDERS[i]\n",
    "    dataset.insert(dataset.columns.size, \"PROVIDER\", provider)\n",
    "\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain null-like string values should be replaced with null\n",
    "for i, dataset in enumerate(data):\n",
    "    null_like = [\"<null>\", \"#REF!\"]\n",
    "\n",
    "    for j in null_like: dataset = dataset.replace(j, np.nan)\n",
    "    \n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with duplicate outage ID and output features\n",
    "for i, dataset in enumerate(data):\n",
    "    dataset = dataset.drop_duplicates(subset=[index, *output_labels], keep='last')\n",
    "    data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine(rows : pd.Series) -> object:\n",
    "#     \"\"\"\n",
    "#     Combine a series of rows from a given column into one item.\n",
    "\n",
    "#     The combination uses different methods based on the data type of the rows:\n",
    "#     - ``str``: Returns a concatenation of all unique values.\n",
    "#     - ``int``: Returns the mode.\n",
    "#     - ``nan``: Returns ``nan``.\n",
    "\n",
    "#     Args:\n",
    "#         rows (pd.Series): The series of rows.\n",
    "\n",
    "#     Returns:\n",
    "#         object: The combination of the rows.\n",
    "#     \"\"\"\n",
    "#     # If the series is entirely empty, return NaN\n",
    "#     if rows.value_counts().empty: return np.nan\n",
    "    \n",
    "#     if len(rows) == 1: return rows.iloc[0]\n",
    "\n",
    "#     # If the series is made of strings:\n",
    "#     if pd.api.types.is_string_dtype(rows):\n",
    "        \n",
    "#         # Concatenate all unique strings\n",
    "#         unique_values = rows.unique()\n",
    "#         return \", \".join(unique_values)\n",
    "\n",
    "#         return rows.loc[rows.str.len().idxmax()]\n",
    "#     else:\n",
    "#         # Otherwise, return the mode\n",
    "#         return rows.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For rows with duplicate Outage IDs,\n",
    "# # concatenate input features when the output labels are the same,\n",
    "# # or split them into distinct cases when the output labels are different.\n",
    "# # (Combine all rows with duplicate outage IDs and output labels.)\n",
    "\n",
    "# for i, dataset in enumerate(data):\n",
    "#     dataset = dataset.groupby([index, *output_labels]).agg(combine).reset_index()\n",
    "#     data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all datasets into one\n",
    "dataset = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make spacing / capitalisation of MSSS feature labels consistent\n",
    "dataset[output_labels] = dataset[output_labels].map(lambda x: x.strip().title() if type(x) is str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct inconsistent capitalisation within MSSS feature labels where possible\n",
    "\n",
    "def map_item_to_msss_feature_label(item : str, label_names : list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Given a MSSS label (e.g., \" animal\"), try to map\n",
    "    it to an MSSS label from a list (e.g., \"Animal\")\n",
    "    to correct issues in capitalisation / spacing.\n",
    "\n",
    "    Args:\n",
    "        item (str): The raw label name.\n",
    "        label_names (list[str]): List of legal label names.\n",
    "\n",
    "    Returns:\n",
    "        label (str | None): The correct label name, if found, otherwise the raw label name.\n",
    "    \"\"\"\n",
    "    if pd.isna(item): return np.nan\n",
    "\n",
    "    if item in label_names: return item\n",
    "\n",
    "    original_item = item\n",
    "    item = item.strip().lower()\n",
    "\n",
    "    labels_easy = [i.strip().lower() for i in label_names]\n",
    "    if item in labels_easy:\n",
    "        index = labels_easy.index(item)\n",
    "        return label_names[ index ]\n",
    "\n",
    "    word_matches = [i for i in labels_easy if item in i]\n",
    "    \n",
    "    if word_matches:\n",
    "        if len(word_matches) == 1:\n",
    "            item = word_matches[0]\n",
    "            index = labels_easy.index(item)\n",
    "            return label_names[ index ]\n",
    "    \n",
    "    return original_item\n",
    "\n",
    "def rectify_feature(dataset : pd.DataFrame, msss_feature : str, msss_table : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Correct an MSSS feature from a dataset by removing inconsistencies in capitalisation / spacing.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The original dataset\n",
    "        msss_feature (str): MSSS feature name, e.g., \"Cause\".\n",
    "        msss_table (pd.DataFrame): MSSS lookup table.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with msss_feature rectified.\n",
    "    \"\"\"\n",
    "    table_column = f\"{msss_feature} Description\"\n",
    "    data_column = f\"MSSS_{msss_feature.upper()}_DESCRIPTION\"\n",
    "\n",
    "    label_names = msss_table[table_column].unique().tolist()\n",
    "\n",
    "    dataset[data_column] = dataset[data_column].map(lambda x: map_item_to_msss_feature_label(x, label_names))\n",
    "    return dataset\n",
    "\n",
    "for feature in [\"Object\", \"Cause\", \"Damage\"]:\n",
    "    dataset = rectify_feature(dataset, feature, msss_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Use an LLM to map each unknown MSSS label to one in the specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain all unknown labels\n",
    "Get all labels in the dataset which are NOT present in the MSSS specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unknowns(dataset, msss_feature, msss_lookup_table):\n",
    "\n",
    "    table_column = f\"{msss_feature} Description\"\n",
    "    data_column = f\"MSSS_{msss_feature.upper()}_DESCRIPTION\"\n",
    "\n",
    "    label_names = msss_lookup_table[table_column].unique().tolist()\n",
    "\n",
    "    items = dataset[data_column].dropna().unique().tolist()\n",
    "    unknown_items = [i for i in items if i not in label_names]\n",
    "\n",
    "    return unknown_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_labels = {\n",
    "    \"Object\" : get_unknowns(dataset, \"Object\", msss_table),\n",
    "    \"Damage\" : get_unknowns(dataset, \"Damage\", msss_table),\n",
    "    \"Cause\" : get_unknowns(dataset, \"Cause\", msss_table)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "sys.path.append(\"src/\")\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# Import modules for LLM finetuning and evaluation\n",
    "import finetune as ft\n",
    "import evaluate as ev\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MODEL_DEVICE = \"cuda:0\"\n",
    "QUANTIZED = True # Load model with 4-bit quantization\n",
    "\n",
    "# Same quantization configuration as QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.float16\n",
    ") if QUANTIZED else None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=MODEL_DEVICE,\n",
    "    use_cache=False # use_cache is incompatible with gradient checkpointing\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLM to assign each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_lookup = {}\n",
    "\n",
    "for label, values in unknown_labels.items():\n",
    "\n",
    "    label_names = msss_table[f\"{label} Description\"].unique().tolist()\n",
    "    label_names.append(\"Unknown\")\n",
    "    \n",
    "    label_lookup = []\n",
    "\n",
    "    for id, unknown_label in enumerate(values):\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert at providing Power Outage {label} Description labels from a list: {label_names}.\n",
    "Find a label from the list which most closely matches the user's query.\n",
    "\"\"\".strip()\n",
    "    \n",
    "        chat = [\n",
    "            {\"role\":\"system\", \"content\":prompt},\n",
    "            {\"role\":\"user\", \"content\":unknown_label.strip().title()}\n",
    "        ]\n",
    "        \n",
    "        response = ft.generate(chat, model, tokenizer, max_new_tokens=50)\n",
    "\n",
    "        response_label_id = ev._get_class_id_from_model_response(response, label_names)\n",
    "\n",
    "        # If no label in response:\n",
    "        if response_label_id == len(label_names) - 1:\n",
    "            warnings.warn(f\"LLM could not categorise label: {unknown_label}\\nResponse:\\n{response}.\\nProceeding with Unknown.\")\n",
    "            response_label = \"Unknown\"\n",
    "        else:\n",
    "            response_label = label_names[response_label_id]\n",
    "\n",
    "        print(f\"Mapped {unknown_label} -> {response_label}\")\n",
    "\n",
    "        label_lookup.append(\n",
    "            {\"Old\" : unknown_label,\n",
    "            \"New\" : response_label,\n",
    "            \"Rationale\" : response}\n",
    "        )\n",
    "\n",
    "        # if id > 5: break\n",
    "    \n",
    "    label_lookup = pd.DataFrame(label_lookup)\n",
    "    labels_lookup[label] = label_lookup\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save lookup table to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, lookup_table in labels_lookup.items():\n",
    "\n",
    "    path = os.path.join(EQ_DATA_PATH, f\"../label_{label.lower()}_lookup.csv\")\n",
    "\n",
    "    lookup_table.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map unknown labels to known labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = copy(dataset)\n",
    "for label, lookup_table in labels_lookup.items():\n",
    "\n",
    "    column = f\"MSSS_{label.upper()}_DESCRIPTION\"\n",
    "    lookup_dict = lookup_table.set_index(\"Old\")[\"New\"].to_dict()\n",
    "\n",
    "    dataset2[column] = dataset2[column].map(lambda x: lookup_dict[x] if x in lookup_dict else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed.csv\")\n",
    "\n",
    "dataset.to_csv(new_path, index=False)\n",
    "\n",
    "new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed-supervised.csv\")\n",
    "\n",
    "dataset.dropna(subset=output_labels).to_csv(new_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_item_to_msss_feature_label(item, label_names):\n",
    "    \n",
    "#     if pd.isna(item): return None\n",
    "\n",
    "#     if item in label_names: return item\n",
    "\n",
    "#     item = item.strip().lower()\n",
    "\n",
    "#     labels_easy = [i.strip().lower() for i in label_names]\n",
    "#     if item in labels_easy:\n",
    "#         index = labels_easy.index(item)\n",
    "#         return label_names[ index ]\n",
    "\n",
    "#     word_matches = [i for i in labels_easy if item in i]\n",
    "    \n",
    "#     if word_matches:\n",
    "#         if len(word_matches) == 1:\n",
    "#             item = word_matches[0]\n",
    "#             index = labels_easy.index(item)\n",
    "#             return label_names[ index ]\n",
    "    \n",
    "#     return None\n",
    "\n",
    "# def is_unknown(item, label_names):\n",
    "#     if pd.isna(item): return False\n",
    "\n",
    "#     match = map_item_to_msss_feature_label(item, label_names)\n",
    "#     return (match is None)\n",
    "    \n",
    "# def get_unknowns(dataset, msss_feature):\n",
    "\n",
    "#     table_column = f\"{msss_feature} Description\"\n",
    "#     data_column = f\"MSSS_{msss_feature.upper()}_DESCRIPTION\"\n",
    "\n",
    "#     label_names = msss_table[table_column].unique().tolist()\n",
    "\n",
    "#     items = dataset[data_column]\n",
    "#     unknown_indices = items.map(lambda x: x in label_names)\n",
    "\n",
    "#     unknown_items = items[unknown_indices].unique().tolist()\n",
    "\n",
    "#     return unknown_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dupes = data[1][ data[1].duplicated(keep=False) ]\n",
    "\n",
    "# # Obtain a 2D list containing all rows in the dataset with duplicate indices grouped by index.\n",
    "# # Source: https://stackoverflow.com/a/46629549\n",
    "# duplicate_indices = dupes.groupby(index).apply(lambda x : list(x.index), include_groups=False).tolist()\n",
    "\n",
    "# for indices in duplicate_indices:\n",
    "\n",
    "#     # For all rows which have the same Outage ID:\n",
    "#     rows = data[1].iloc[indices]\n",
    "\n",
    "#     # Combine all columns in the rows into one.\n",
    "#     for column in data[1].columns:\n",
    "        \n",
    "#         values = rows[column]\n",
    "\n",
    "#         combined = values.dropna().mode()[0]\n",
    "\n",
    "#         # Replace the first index with the combined rows\n",
    "#         first_index = indices[0]\n",
    "#         #data[1].loc[index, column] = combined\n",
    "#         data[1][column][first_index] = combined\n",
    "\n",
    "# data[1] = data[1].drop_duplicates(subset=index,keep='first')\n",
    "# #data[1] = data[1].drop(rows_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the index for each dataset\n",
    "# for i, dataset in enumerate(data):\n",
    "#     dataset = dataset.reset_index(drop=False)\n",
    "#     data[i] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate each sample's input features into strings\n",
    "\n",
    "# # Get all input features present in the dataset\n",
    "# inputs = [f for f in input_features if f in dataset.columns]\n",
    "\n",
    "# # Replace all missing input feature text with \"Unknown\"\n",
    "# dataset[inputs] = dataset[inputs].fillna(\"Unknown\")\n",
    "\n",
    "# # Concatenate each sample's input features into strings\n",
    "# input_texts = []\n",
    "# for _, sample in dataset[inputs].iterrows():\n",
    "\n",
    "#     text = [f\"{key}: {value}\" for key, value in sample.items()]\n",
    "#     text = \";\\n\".join(text)\n",
    "#     input_texts.append(text)\n",
    "\n",
    "# # Add all concatenated inputs as a new feature\n",
    "# dataset[\"TEXT\"] = input_texts\n",
    "\n",
    "# # Delete individual input features\n",
    "# dataset = dataset.drop(columns=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_path = os.path.join(EQ_DATA_PATH, \"../preprocessed.csv\")\n",
    "\n",
    "# dataset.to_csv(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"data/eq/datasets/preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
