{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66057dd0-3155-4ca5-b7d3-4087416a4b99",
   "metadata": {},
   "source": [
    "# ▶️ Configure GPUs + Root Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604a738-c56d-4539-8813-d3506e872132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "ROOT_DIR = \"data/eq/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44078c-dcfc-487f-80af-56e2dbc04682",
   "metadata": {},
   "source": [
    "# ▶️ Load and Preprocess OSHA Injuries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c19ec-61ce-415e-9251-4e396ba71ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for LLM finetuning and evaluation\n",
    "import finetune as ft\n",
    "import evaluate as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b64b75-1eda-4189-a2aa-eea6d1436ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34660a-9f23-47fc-8478-3268c68db277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"dataset/preprocessed.csv\"),\n",
    "    low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2393f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna() # Get only the labelled portion of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958f4ab-8e51-4186-a009-473847765c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame into a Dataset\n",
    "dataset = ft.create_dataset_from_dataframe(data, \"TEXT\", \"MSSS_DAMAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6843a3-ae6a-4c5f-9702-9be2d7b4bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset into a form usable for supervised finetuning\n",
    "dataset, label_names = ft.preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8fe57-8936-4b78-a75b-1c31b8cedc1e",
   "metadata": {},
   "source": [
    "# ▶️ Load Baseline LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b13ee8-e353-4f7d-b0d1-0da90ca1d26a",
   "metadata": {},
   "source": [
    "NOTE: I want to refactor this into one function -> ``ft.load_model(name, device_map, quantized)``\n",
    "\n",
    "I think it would be a bit nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d26a1-5028-4f82-bfa4-f5ad3ebaed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MODEL_DEVICE = \"cuda:0\"\n",
    "QUANTIZED = True # Load model with 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721a682-05ab-47c9-81ad-c17c28596e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Same quantization configuration as QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.float16\n",
    ") if QUANTIZED else None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=MODEL_DEVICE,\n",
    "    use_cache=False # use_cache is incompatible with gradient checkpointing\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee3cf1-328f-4f9f-a3ad-7a0cd6fb057a",
   "metadata": {},
   "source": [
    "# ▶️ Evaluate Baseline LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa34ff2-b1a4-45cc-a1b5-b34ef77d0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_prompts as prompts\n",
    "from evaluate import EvaluationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490271f-82e2-4c33-abb0-5f1cb695436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the baseline LLM\n",
    "baseline_configurations = [\n",
    "    EvaluationConfig(\n",
    "        name=\"Zero-shot\",\n",
    "        prompt=prompts.OSHA[\"ZERO_SHOT\"],\n",
    "        max_tokens=3\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f0f8e-3fe6-4aa6-88a2-abaf37506d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(configurations, model, tokenizer, label_names, eval_dataset):\n",
    "    results = []\n",
    "    for config in configurations:\n",
    "        result = ev.evaluate(\n",
    "            model=model, tokenizer=tokenizer, label_names=label_names,\n",
    "            eval_dataset=dataset['test'], eval_config=config\n",
    "        )\n",
    "        results.append(result)\n",
    "        result.save(os.path.join(ROOT_DIR, \"results\")) # Saves to \"data/osha/results/<EvaluationConfig.name>\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e14f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = evaluate_model(baseline_configurations, model=model, tokenizer=tokenizer, label_names=label_names, eval_dataset=dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0eaf7f",
   "metadata": {},
   "source": [
    "# ▶️ Finetune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7396b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_LLM_PATH = os.path.join(ROOT_DIR, \"models/Qwen2.5-FT-OSHA-Injuries-2\")\n",
    "\n",
    "LORA_RANK_DIMENSION = 6 # the rank of the adapter, the lower the fewer parameters you'll need to train. (smaller = more compression)\n",
    "LORA_ALPHA = 8 # this is the scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "LORA_DROPOUT = 0.05 # dropout probability for LoRA layers (helps prevent overfitting)\n",
    "MAX_SEQ_LENGTH = 64\n",
    "EPOCHS=1\n",
    "LEARNING_RATE=2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd13482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK_DIMENSION,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    auto_find_batch_size=True,\n",
    "    \n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=True,\n",
    "    \n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim='adamw_torch_fused',\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\", \n",
    "    \n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir=FINETUNED_LLM_PATH,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.finetune( # Will save the model to the directory: FINETUNED_LLM_PATH\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'],\n",
    "    lora_config=lora_config, sft_config=sft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ef836",
   "metadata": {},
   "source": [
    "# ▶️ Load Finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the baseline model if it exists, otherwise we will probably get an OOM exception\n",
    "import gc, torch\n",
    "\n",
    "if \"bnb_config\" in locals(): del bnb_config\n",
    "if \"tokenizer\" in locals(): del tokenizer\n",
    "if \"model\" in locals(): del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8909f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_LLM_PATH = os.path.join(ROOT_DIR, \"models/Qwen2.5-FT-OSHA-Injuries-2\")\n",
    "MODEL_DEVICE = \"cuda:0\"\n",
    "QUANTIZED = True # Load model with 4-bit quantization\n",
    "\n",
    "model, tokenizer = ft.load_finetuned_llm(FINETUNED_LLM_PATH, MODEL_DEVICE, QUANTIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed475c01",
   "metadata": {},
   "source": [
    "# ▶️ Evaluate Finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7afae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_configurations = [\n",
    "    EvaluationConfig(\n",
    "        name=\"Fine-tuned\",\n",
    "        prompt=None,\n",
    "        max_tokens=3\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = evaluate_model(finetuned_configurations, model=model, tokenizer=tokenizer, label_names=label_names, eval_dataset=dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
